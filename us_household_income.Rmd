---
title: "US_Household_Income"
author: "Naeem Chowdhury, Rosa Gradilla"
date: "December 2, 2019"
output: html_document
editor_options: 
  chunk_output_type: inline
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
knitr::opts_chunk$set(tidy=TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=70))
knitr::opts_chunk$set(fig.height=4, fig.width=6)
```


## libraries

```{r libraries}
#Load in needed libraries 
# Run these if you don't have these libraries yet
# install.packages("tidyverse") 
# install.packages("data.table")
# install.packages("plotly") #For pretty plots
library(tidyverse) # For data processing and visualization
library(data.table) # For creating tables
library(plotly)
library(ggplot2)
library(reshape2)
spot_color = "#9ecae1" 
text_color = "#525252"
```

## File management

```{r file_management}
#Create variables for directories
home.dir <- getwd()
data.dir <- './rawData'
viz.dir <- './dataViz'
output.dir <- './output'
```

## Preprocessing

```{r}
#import
df_income <- read_csv(file.path(home.dir, data.dir, 'kaggle_income.csv'), na = c(''))

df_income2 <- df_income %>% filter(Median != 300000 & Median != 0)
```

### Exploration (Playground)

## Quantitative Variables
Mean, Median, St. Dev, Sum of Weights, AWater, ALand


### Histograms
```{r}
#number 1
# These histograms satisfy the distribution requirement of the project.
ggplot(data = df_income2)+
  geom_histogram(mapping = aes(x = df_income2$Mean, fill = df_income2$Type), bins = 40) +
  labs(x = "Household Income (Mean)") +
  scale_fill_discrete(name = "Household Type")
```

```{r}
# Distribution
ggplot(data = df_income)+
  geom_histogram(mapping = aes(x = df_income$Median, fill = df_income$Type), bins = 40) +
  labs(x = "Household Income (Median)") +
  scale_fill_discrete(name = "Household Type")
```


### What's going on with all the rich people?
There are 4k+ data points which have precisely 300000 dollars as their median income value. This is a data anomaly that messes with our summary statistics, so we decided it would be best to toss them out.
```{r}
# There are 4k+ data points which have precisely 300000 dollars as their median income value. This is a data anomaly that messes with our summary statistics, so we decided it would be best to toss them out.
rich_people <- df_income %>% filter(Median == 300000)

count(rich_people)
```


### Analysis

## Correlation Matrix

```{r}
quant_vars <- df_income %>% 
  select(Mean, Median, Stdev, AWater, ALand)

cor_quant <- cor(quant_vars)

melted_quant <- melt(cor_quant)

ggplot(data = melted_quant, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()
```

## Scatter Plots and Linear Regressions
```{r}
# An initial scatter plot to see the shape of the relationship
ggplot(quant_vars, aes(x=Median, y=AWater)) + geom_point()
ggplot(quant_vars, aes(x=Median, y=ALand)) + geom_point()


```
It seems that the correlations we produced in the previous section are a result of outlier values that skew the data. Lets try throwing out the outliers.


### Data Reshaping to get rid of Problem Points

##Correlation matrix without outliers

```{r}
df_income <- df_income %>% filter(Median != 300000 & Median != 0)

quant_vars <- df_income %>% 
  select(Mean, Median, Stdev, AWater, ALand)

shaved_data <- quant_vars %>% filter(Median < 150000 & ALand < 30000000)
ggplot(shaved_data, aes(x=Median, y=ALand)) + geom_point()

cor_quant <- cor(shaved_data)

melted_quant <- melt(cor_quant)

ggplot(data = melted_quant, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()

```

## Histogram without outliers
```{r}
# Distribution
ggplot(data = df_income)+
  geom_histogram(mapping = aes(x = df_income$Median, fill = df_income$Type), bins = 40) +
  labs(x = "Household Income (Median)") +
  scale_fill_discrete(name = "Household Type")
```

### One-Sample Statistical Inference for Income

Confidence interval for income's population mean income value.
```{r}
#-Construct a confidence interval for income's population mean value. (Make sure to check assumptions and interpret the CI)
#-Conduct a hypothesis test against a certain value. Interpret its results

#number 2
clevel <- 0.95
n <- length(df_income2$Median)
  
xbar <- mean(df_income2$Median)
s <- sd(df_income2$Median)
se <- s/(sqrt(n))
x <- (1-clevel)/2
p <- clevel + x
cv <- qt(p, n-1)
CI <- c(xbar - (cv*se), xbar + (cv*se))

print(CI)


#Hypothesis Test
#Ho: mu = 50,000      #Ha: mu != 50,000
t.test(df_income2$Median, mu=50000, alternative = "two.sided")
```
We tested against the null hypothesis mu = 50,000 on a two sided t test. Our confidence interval for population's median income is [53145.03, 53812.59]. We are 95% confident the true population's median income falls bewteen our confidence interval 95% of the time. Given that our data came from a random sample and assuming the population's distribution is approximately normal.

We came up with a null hypothesis claiming the true population's median income is equal to 50,000, our t-test against this null hypothesis gave us a p-value < 2.2e-16 thus we reject the null hypothesis. This agrees with our confidence interval given that 50,000 is not part of it.

```{r}
#Cohen's d
d <- abs(xbar - 50000)/s
d

```
Our effect size is 0.1222492. We witness a relatively small practical effect size.

### Categorical Explanatory Variable

Here we are performing a proportion test for the proportion of median income being lower than 50,000
```{r}
#number 3
cuts <- quantile(df_income2$Median, c(0, 1/2, 1))

df_categ <- df_income2 %>% mutate(category=cut(Median, breaks=cuts, labels=c("below","above")))

shaved_income <- df_categ %>% select(category)
table(shaved_income)

prop.table <- table(shaved_income)*(1/27924)

# H_0: The proportion of areas with median income below $50k is 0.5.
# H_a: The proportion of areas with median income below $50k is greater than or less than 0.5.

prop.test(13954, 27924, 0.5)
```
Our confidence interval is [0.4938316, 0.5055955]. We tested a null p equal to 0.5. We failed to reject the null hypothesis. A p-value of 0.9285 indicates that the probability of observing our test statistic p = 0.4997135 is 93% given the null-hypothesis is true.

```{r}
#Cohen's h
x <- asin(sqrt(0.4997135)) - asin(sqrt(0.5))
effect.size <- 2*abs(x)
```
Our effect size is 0.000573, we witness a very small practical effect size.

### Two-sample statistical inference
Two sample: compare means of two population groups: cities and towns

Is there a significant difference between the household incomes in cities and towns?

Ho: mt = mc  i.e. there is no difference
Ha: mt != mc

```{r}
#number 4
#Check assumptions, interpret the results.

city <- df_income2 %>%
    filter(Type == 'City')

town <- df_income2 %>%
  filter(Type == 'Town')

t.test(city$Median, town$Median)
```
95% confidence interval is [747.1329, 5865.2244]. P-value is equal to 0.0114, at a 0.05 significance level we fail to reject the null hypothesis: the true difference in means is not equal to 0. This agrees with our conficence inteval since 0 is not within it.

Checking assumptions:
```{r}
hist(city$Mean)
hist(town$Mean)
```
Since both of our groups seem to be slightly skwed, out t-test might not be reliable since it assumes the data is normally distributed.

Effect size:
```{r}
s1 <- sd(town$Median)
s2 <- sd(city$Median)
effect.size <- abs(47112.28 - 43806.10)/sqrt((s1^2 + s2^2)/2)
```
Our effect size is 0.148239. thus we witness a relatively small effect size. 


For the confidence interval and hypothesis test conducted, we ran simulations: 
we took a random subsample of fixed size (n=100) from our full data set, we then proceeded to calculate the confidence intervals and hypothesis tests against the true sample mean, recording the coverage.

```{r}
#number 5
x <- df_income2$Median
mu <- mean(x)

n.trials <- 10000
n <- 100
inside <-  numeric(n.trials)
for (j in 1:n.trials){
  x.samp <- sample(x, n)
  x.bar <- mean(x.samp)
  x.sd <- sd(x.samp)
  CI[j] <- (mu > x.bar - qt(0.95,n-1)*x.sd/sqrt(n) & mu < x.bar + qt(0.95,n-1)*x.sd/sqrt(n))
  inside[j] <- CI[j]
}

coverage <- mean(inside)
print(coverage)
```
Our coverage for the true sample mean over 10,000 trials was 89.60%, a little lower than expected (95%), this could be attributed to outliers in the data. 
